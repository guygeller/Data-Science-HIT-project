{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for the request to represent it was requested by a browser\n",
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"96\", \"Google Chrome\";v=\"96\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Accept-Language': 'he-IL,he;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "\n",
    "# Funtion to make 'get' request for the url and return it as a soup \n",
    "def load_soup_object(url_path):\n",
    "    page = requests.get(url_path, headers = headers)\n",
    "    #print(page)\n",
    "    return BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488418de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to messure the time spent for crawling\n",
    "def time_convert(sec):\n",
    "    mins = sec // 60\n",
    "    sec = sec % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    print(\"Time Lapsed = {0}:{1}:{2}\".format(int(hours),int(mins),sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the data frame\n",
    "dataframe = pd.DataFrame(columns = ['Name','Marital Status', 'Reality', 'Gender', 'Hair Color',\n",
    "                                    'Eye Color', 'Living status', 'Identity', 'Citenzenship', 'Year', 'Appearnces'])\n",
    "\n",
    "# Url prefix for each link in the page\n",
    "url_prefix = 'https://marvel.fandom.com'\n",
    "\n",
    "# Flag to mark the first page, (the first page contains 18 links that are not character pages)\n",
    "first_page = True\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "#\n",
    "next_page = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56689d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading the first page\n",
    "soup = load_soup_object('https://marvel.fandom.com/wiki/Category:Characters')\n",
    "\n",
    "# Flag to represent if there is still a \"Next\" page.\n",
    "not_last_page = True\n",
    "while not_last_page:\n",
    "    \n",
    "    # Wrapper that hold all the links in the page\n",
    "    current_page_character_list = soup.find_all('div', {'class': 'category-page__members-wrapper'})\n",
    "    \n",
    "    # Soup with all the links in the current page\n",
    "    if first_page:\n",
    "        links = soup.find_all('a',  {'class': 'category-page__member-link'})[18:]\n",
    "        first_page = False\n",
    "    else:\n",
    "        links = soup.find_all('a',  {'class': 'category-page__member-link'})\n",
    "\n",
    "    # Loop that runs on all links that a page has    \n",
    "    for i,X in enumerate(links):\n",
    "        # Create new list to represent the each row of the data.\n",
    "        row = list()\n",
    "        #print(str(i) + \". \" + url_prefix + X.get('href'))\n",
    "        \n",
    "        #Try and CCatch statement incase the soup loading failed.\n",
    "        try:\n",
    "            character_page_soup = load_soup_object(url_prefix + X.get('href'))\n",
    "        except:\n",
    "            print(\"An exception occurred during loading character page\")\n",
    "\n",
    "         # Adding the name of the character  \n",
    "        if character_page_soup.find('div',  {'data-source': 'Name'}) is not None:\n",
    "            if character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'}).a is not None:\n",
    "                name = character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'}).a\n",
    "                row.append(name.get_text().strip())\n",
    "            else:\n",
    "                name = character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'})\n",
    "                name.get_text().strip()\n",
    "                row.append(name.get_text().strip())\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding the Marital Status of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'MaritalStatus'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'MaritalStatus'}).a is not None: \n",
    "                m_status = character_page_soup.find('div', {'data-source': 'MaritalStatus'}).a\n",
    "                row.append(m_status.get_text().strip())\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding the Reality of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Reality'}) is not None: \n",
    "            reality_content = character_page_soup.find('div',  {'data-source': 'Reality'}).find('div',  {'class': 'pi-data-value pi-font'}).get_text().strip()\n",
    "            row.append(reality_content)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding gender of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Gender'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Gender'}).a is not None:\n",
    "                gender = character_page_soup.find('div', {'data-source': 'Gender'}).a\n",
    "                row.append(gender.get_text().strip())\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding hair color of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Hair'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Hair'}).a is not None:\n",
    "                hair = character_page_soup.find('div', {'data-source': 'Hair'}).a\n",
    "                row.append(hair.get_text().strip())\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding eye color of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Eyes'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Eyes'}).a is not None:\n",
    "                eye = character_page_soup.find('div', {'data-source': 'Eyes'}).a\n",
    "                row.append(eye.get_text().strip())\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding living status of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Status'}) is not None:\n",
    "            status = character_page_soup.find('div',{'data-source': 'Status'}).find_all('a')\n",
    "            row.append(status[0].get_text().strip())\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding Identity status of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Identity'}) is not None:\n",
    "            if character_page_soup.find('div',  {'data-source': 'Identity'}).a is not None:\n",
    "                identity =  character_page_soup.find('div',  {'data-source': 'Identity'}).a.get_text().strip()\n",
    "                row.append(identity)\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "            \n",
    "        # Adding Citizenship of the character\n",
    "        if character_page_soup.find('div',  {'data-source': 'Citizenship'}) is not None:\n",
    "            citizen_content = character_page_soup.find('div',  {'data-source': 'Citizenship'}).find('div',  {'class': 'pi-data-value pi-font'}).get_text().strip()\n",
    "            row.append(citizen_content)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # Adding Year of the character first apearence\n",
    "        if character_page_soup.find('div',  {'data-source': 'First'}) is not None:\n",
    "            year_content = character_page_soup.find('div', {'data-source': 'First'}).find_all('a')\n",
    "            if(len(year_content) > 1):\n",
    "                year = character_page_soup.find('div', {'data-source': 'First'}).find_all('a')[1].get_text().strip().split()[-1]\n",
    "            else:\n",
    "                if len(character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()) > 0:\n",
    "                    year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()[-1]\n",
    "                    if 'Mentioned' in year:\n",
    "                        year = year[0:4]\n",
    "                    elif 'Carving' in year:\n",
    "                        year = year[0:4]\n",
    "                    elif 'Series' in year:\n",
    "                        year = year[0:4]\n",
    "                    else:\n",
    "                        year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()[-1][:-1]\n",
    "                else:\n",
    "                    year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip()\n",
    "            row.append(year)\n",
    "        else:\n",
    "            row.append(None)\n",
    "            \n",
    "            \n",
    "        # Adding the number of appearences of the character in comic books\n",
    "        if character_page_soup.find('span',  {'id': 'See_Also'}) is not None:\n",
    "            appearnces_parent = character_page_soup.find('span', {'id': 'See_Also'}).parent\n",
    "            if appearnces_parent.next_sibling.next_sibling.find('li') is not None:\n",
    "                appearnces_content = appearnces_parent.next_sibling.next_sibling.find('li').get_text().strip().split()[0]\n",
    "                row.append(appearnces_content.split()[0])\n",
    "            else:\n",
    "                row.append(None)\n",
    "        else:\n",
    "            row.append(None)\n",
    "\n",
    "        # appending the row to the dataframe\n",
    "        series = pd.Series(row, index = dataframe.columns)\n",
    "        dataframe = dataframe.append(series, ignore_index=True)\n",
    "        \n",
    "        # writing to the directly to a diffrent CSV for a backup\n",
    "        with open(r'C:\\Users\\guyge\\Desktop\\data science\\Marvel HIT\\Data-Science-HIT-project\\Marvel CSV.csv', 'a', newline='') as f_object:  \n",
    "            # Pass the CSV  file object to the writer() function\n",
    "            writer_object = writer(f_object)\n",
    "            # Result - a writer object\n",
    "            # Pass the data in the list as an argument into the writerow() function\n",
    "            try:\n",
    "                writer_object.writerow(row)\n",
    "            except:\n",
    "                print(\"Didn't write a row to csv\")\n",
    "            # Close the file object\n",
    "            f_object.close()\n",
    "\n",
    "   # Opening the next page\n",
    "    if soup.find('a', {'class':'category-page__pagination-next'}) is not None:\n",
    "        next_page = soup.find('a', {'class':'category-page__pagination-next'}).get('href')\n",
    "        try:\n",
    "            soup = load_soup_object(next_page)\n",
    "        except:\n",
    "            print(\"An exception occurred during loading next page\")\n",
    "    \n",
    "    else:\n",
    "        not_last_page = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the last page the crawler was on in case the was an exception,\n",
    "# so we can continue from where it ended\n",
    "print(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d458b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time and print the timer\n",
    "end_time = time.time()\n",
    "time_lapsed = end_time - start_time\n",
    "time_convert(time_lapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df919f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the dataframe to CSV.\n",
    "dataframe.to_csv('Marvel Dataframe 2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
