{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce0386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import re\n",
    "from fp.fp import FreeProxy\n",
    "from lxml.html import fromstring\n",
    "from itertools import cycle\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6682c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxies = list()\n",
    "# for i in range(10):\n",
    "#     proxy = FreeProxy().get()\n",
    "#     proxies.append(proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e8e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "#     response = requests.get(url)\n",
    "#     parser = fromstring(response.text)\n",
    "#     proxies = set()\n",
    "#     for i in parser.xpath('//tbody/tr')[:10]:\n",
    "#         if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "#             #Grabbing IP and corresponding PORT\n",
    "#             proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "#             proxies.add(proxy)\n",
    "#     return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy_pool = cycle(proxies)\n",
    "# url = 'https://httpbin.org/ip'\n",
    "# for i in range(1,11):\n",
    "#     #Get a proxy from the pool\n",
    "#     proxy = next(proxy_pool)\n",
    "#     print(\"Request #%d\"%i)\n",
    "#     try:\n",
    "#         response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#         print(response.json())\n",
    "#     except:\n",
    "#     #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "#     #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "#         print(\"Skipping. Connnection error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a982dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Connection': 'keep-alive',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"96\", \"Google Chrome\";v=\"96\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Accept-Language': 'he-IL,he;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "\n",
    "\n",
    "def load_soup_object(url_path):\n",
    "    sleep(randint(3,5))\n",
    "    page = requests.get(url_path, headers = headers)\n",
    "    #print(page)\n",
    "    return BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488418de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## name, current alias, affiliation, marital status, gender,\n",
    "## height, weight, hair color, origin, living status, cause of death, reality, place of birth,\n",
    "## identity, citenzenship, education, creators, unusual features, occupation\n",
    "\n",
    "\n",
    "def time_convert(sec):\n",
    "    mins = sec // 60\n",
    "    sec = sec % 60\n",
    "    hours = mins // 60\n",
    "    mins = mins % 60\n",
    "    print(\"Time Lapsed = {0}:{1}:{2}\".format(int(hours),int(mins),sec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns = ['Name','Marital Status', 'Gender', 'Hair Color',\n",
    "                                    'Eye Color', 'Living status', 'Reality','Identity',\n",
    "                                    'Citenzenship','Appearnces', 'Year'])\n",
    "\n",
    "name_lst = [] \n",
    "marital_status_lst = [] \n",
    "gender_lst = []\n",
    "hair_color_lst = [] \n",
    "eye_color_lst = [] \n",
    "living_status_lst = []\n",
    "reality_lst = []\n",
    "identity_lst = [] \n",
    "citenzenship_lst = [] \n",
    "appearnces_lst = []\n",
    "year_lst = []\n",
    "\n",
    "url_prefix = 'https://marvel.fandom.com'\n",
    "first_page = True\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56689d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An exception occurred during loading character page\n",
      "An exception occurred during loading character page\n",
      "An exception occurred during loading character page\n",
      "An exception occurred during loading character page\n"
     ]
    }
   ],
   "source": [
    "soup = load_soup_object('https://marvel.fandom.com/wiki/Category:Characters')\n",
    "not_last_page = True\n",
    "while not_last_page:\n",
    "    current_page_character_list = soup.find_all('div', {'class': 'category-page__members-wrapper'})\n",
    "    if first_page:\n",
    "        links = soup.find_all('a',  {'class': 'category-page__member-link'})[18:]\n",
    "        first_page = False\n",
    "    else:\n",
    "        links = soup.find_all('a',  {'class': 'category-page__member-link'})\n",
    "\n",
    "    for i,X in enumerate(links):     \n",
    "        #print(str(i) + \". \" + url_prefix + X.get('href'))\n",
    "        try:\n",
    "            character_page_soup = load_soup_object(url_prefix + X.get('href'))\n",
    "        except:\n",
    "            print(\"An exception occurred during loading character page\")\n",
    "            #pass\n",
    "        if character_page_soup.find('div',  {'data-source': 'Name'}) is not None:\n",
    "            if character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'}).a is not None:\n",
    "                name = character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'}).a\n",
    "                name_lst.append(name.get_text().strip())\n",
    "            else:\n",
    "                name = character_page_soup.find('div',  {'data-source': 'Name'}).find('div',  {'class': 'pi-data-value pi-font'})\n",
    "                name.get_text().strip()\n",
    "                name_lst.append(name.get_text().strip())\n",
    "        else:\n",
    "            name_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'MaritalStatus'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'MaritalStatus'}).a is not None: \n",
    "                m_status = character_page_soup.find('div', {'data-source': 'MaritalStatus'}).a\n",
    "                marital_status_lst.append(m_status.get_text().strip())\n",
    "            else:\n",
    "                marital_status_lst.append(None)\n",
    "        else:\n",
    "            marital_status_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Reality'}) is not None: \n",
    "            reality_content = character_page_soup.find('div',  {'data-source': 'Reality'}).find('div',  {'class': 'pi-data-value pi-font'}).get_text().strip()\n",
    "            reality_lst.append(reality_content)\n",
    "        else:\n",
    "            reality_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Gender'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Gender'}).a is not None:\n",
    "                gender = character_page_soup.find('div', {'data-source': 'Gender'}).a\n",
    "                gender_lst.append(gender.get_text().strip())\n",
    "            else:\n",
    "                gender_lst.append(None)\n",
    "        else:\n",
    "            gender_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Hair'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Hair'}).a is not None:\n",
    "                hair = character_page_soup.find('div', {'data-source': 'Hair'}).a\n",
    "                hair_color_lst.append(hair.get_text().strip())\n",
    "            else:\n",
    "                hair_color_lst.append(None)\n",
    "        else:\n",
    "            hair_color_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Eyes'}) is not None:\n",
    "            if character_page_soup.find('div', {'data-source': 'Eyes'}).a is not None:\n",
    "                eye = character_page_soup.find('div', {'data-source': 'Eyes'}).a\n",
    "                eye_color_lst.append(eye.get_text().strip())\n",
    "            else:\n",
    "                eye_color_lst.append(None)\n",
    "        else:\n",
    "            eye_color_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Status'}) is not None:\n",
    "            status = character_page_soup.find('div',{'data-source': 'Status'}).find_all('a')\n",
    "            living_status_lst.append(status[0].get_text().strip())\n",
    "        else:\n",
    "            living_status_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Identity'}) is not None:\n",
    "            if character_page_soup.find('div',  {'data-source': 'Identity'}).a is not None:\n",
    "                identity =  character_page_soup.find('div',  {'data-source': 'Identity'}).a.get_text().strip()\n",
    "                identity_lst.append(identity)\n",
    "            else:\n",
    "                identity_lst.append(None)\n",
    "        else:\n",
    "            identity_lst.append(None)\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'Citizenship'}) is not None:\n",
    "            citizen_content = character_page_soup.find('div',  {'data-source': 'Citizenship'}).find('div',  {'class': 'pi-data-value pi-font'}).get_text().strip()\n",
    "            citenzenship_lst.append(citizen_content)\n",
    "        else:\n",
    "            citenzenship_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('div',  {'data-source': 'First'}) is not None:\n",
    "            year_content = character_page_soup.find('div', {'data-source': 'First'}).find_all('a')\n",
    "            if(len(year_content) > 1):\n",
    "                year = character_page_soup.find('div', {'data-source': 'First'}).find_all('a')[1].get_text().strip().split()[-1]\n",
    "            else:\n",
    "                if len(character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()) > 0:\n",
    "                    year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()[-1]\n",
    "                    if 'Mentioned' in year:\n",
    "                        year = year[0:4]\n",
    "                    if 'Carving' in year:\n",
    "                        year = year[0:4]\n",
    "                    if 'Series' in year:\n",
    "                        year = year[0:4]\n",
    "                    else:\n",
    "                        year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip().split()[-1][:-1]\n",
    "                else:\n",
    "                    year = character_page_soup.find('div', {'data-source': 'First'}).get_text().strip()\n",
    "            year_lst.append(year)\n",
    "        else:\n",
    "            year_lst.append(None)\n",
    "\n",
    "\n",
    "        if character_page_soup.find('span',  {'id': 'See_Also'}) is not None:\n",
    "            appearnces_parent = character_page_soup.find('span', {'id': 'See_Also'}).parent\n",
    "            if appearnces_parent.next_sibling.next_sibling.find('li') is not None:\n",
    "                appearnces_content = appearnces_parent.next_sibling.next_sibling.find('li').get_text().strip().split()[0]\n",
    "                appearnces_lst.append(appearnces_content.split()[0])\n",
    "            else:\n",
    "                appearnces_lst.append(None)\n",
    "        else:\n",
    "            appearnces_lst.append(None)\n",
    "       \n",
    "    if soup.find('a', {'class':'category-page__pagination-next'}) is not None:\n",
    "        next_page = soup.find('a', {'class':'category-page__pagination-next'}).get('href')\n",
    "        try:\n",
    "            soup = load_soup_object(next_page)\n",
    "        except:\n",
    "            print(\"An exception occurred during loading next page\")\n",
    "            #pass    \n",
    "    else:\n",
    "         not_last_page = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4505e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "time_lapsed = end_time - start_time\n",
    "time_convert(time_lapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fae09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe['Name'] = name_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Marital Status'] = marital_status_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Gender'] = gender_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Hair Color'] = hair_color_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de137335",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Living status'] = living_status_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6178e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Reality'] = reality_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Identity'] = identity_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d41a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Citenzenship'] = citenzenship_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Eye Color'] = eye_color_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27631bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Appearnces'] = appearnces_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Year'] = year_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df919f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('Marvel Dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2872f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
